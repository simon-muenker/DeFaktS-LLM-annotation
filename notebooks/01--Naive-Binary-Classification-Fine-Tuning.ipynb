{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import datasets\n",
    "\n",
    "import transformers\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE: str = \"../data/processed/DefaktS_Twitter.binary.csv\"\n",
    "TEST_FRAC: float = 0.10\n",
    "\n",
    "MODEL_SLUG: str = \"Twitter/twhin-bert-base\"\n",
    "\n",
    "OUT_DIR: str = \"./fine_tuning_ouput/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>428142</th>\n",
       "      <td>Die Menschen in Belutschistan hören nicht auf,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387854</th>\n",
       "      <td>Im #Iran geht das Regime nicht nur in #Kurdist...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407119</th>\n",
       "      <td>US-Jury spricht Elon #Musk im Betrugsprozess u...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392035</th>\n",
       "      <td>Führende Fachpolitiker von Grünen und SPD im B...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407800</th>\n",
       "      <td>Hyundai Ioniq 6 Electrified Streamliner\\nab 29...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label\n",
       "id                                                              \n",
       "428142  Die Menschen in Belutschistan hören nicht auf,...      0\n",
       "387854  Im #Iran geht das Regime nicht nur in #Kurdist...      0\n",
       "407119  US-Jury spricht Elon #Musk im Betrugsprozess u...      0\n",
       "392035  Führende Fachpolitiker von Grünen und SPD im B...      0\n",
       "407800  Hyundai Ioniq 6 Electrified Streamliner\\nab 29...      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA: pandas.DataFrame = (\n",
    "    pandas.read_csv(DATA_FILE, index_col=[0])\n",
    "    .rename(columns={\"binary_label\": \"label\"})\n",
    "\n",
    "    # remove urls\n",
    "    .pipe(lambda _df: _df.assign(\n",
    "        text=(\n",
    "            _df[\"text\"]\n",
    "            # replace urls with special token\n",
    "            .str.replace(r\"https:\\/\\/t.co\\/\\S+\", \"[URL]\", regex=True)\n",
    "        ),\n",
    "        label=(\n",
    "            _df[\"label\"].astype(int)\n",
    "        )\n",
    "    ))\n",
    "\n",
    "    # downsample to smallest category\n",
    "    .pipe(lambda _df: (\n",
    "        _df\n",
    "        .groupby(\"label\")\n",
    "        .sample(n=min(_df[\"label\"].value_counts()))\n",
    "    ))\n",
    ")\n",
    "DATA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14805, 1645, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_TRAIN = DATA.sample(frac=1.0 - TEST_FRAC)\n",
    "DATA_TEST = DATA.loc[DATA.index.difference(DATA_TRAIN.index)]\n",
    "\n",
    "DATASET_TRAIN = datasets.Dataset.from_pandas(DATA_TRAIN, split=\"train\")\n",
    "DATASET_TEST = datasets.Dataset.from_pandas(DATA_TEST, split=\"test\")\n",
    "\n",
    "len(DATASET_TRAIN), len(DATASET_TEST), DATA_TRAIN.label.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Twitter/twhin-bert-base and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "TOKENIZER = transformers.AutoTokenizer.from_pretrained(MODEL_SLUG)\n",
    "MODEL = transformers.AutoModelForSequenceClassification.from_pretrained(MODEL_SLUG, num_labels=DATA_TRAIN.label.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(sample):\n",
    "    return TOKENIZER(sample[\"text\"], padding=\"max_length\", truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7618e87994b945b582ee10b32c45df1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14805 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56064f374da54074aa1c49f04b4515cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1645 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_tokenized_dataset = DATASET_TRAIN.map(tokenize_function, batched=True)\n",
    "test_tokenized_dataset = DATASET_TEST.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro', zero_division=0.0)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "    }\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=MODEL,\n",
    "    args=transformers.TrainingArguments(\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=32,\n",
    "        output_dir=OUT_DIR,\n",
    "        overwrite_output_dir=True,\n",
    "        save_total_limit=1,\n",
    "        logging_first_step=True,\n",
    "        logging_steps=50,\n",
    "        eval_strategy=\"steps\"\n",
    "    ),\n",
    "    train_dataset=train_tokenized_dataset,\n",
    "    eval_dataset=test_tokenized_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1389' max='1389' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1389/1389 18:29, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.594100</td>\n",
       "      <td>0.484997</td>\n",
       "      <td>0.758055</td>\n",
       "      <td>0.754955</td>\n",
       "      <td>0.775115</td>\n",
       "      <td>0.759614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.486000</td>\n",
       "      <td>0.445908</td>\n",
       "      <td>0.794529</td>\n",
       "      <td>0.792783</td>\n",
       "      <td>0.807643</td>\n",
       "      <td>0.795830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.437300</td>\n",
       "      <td>0.422670</td>\n",
       "      <td>0.795137</td>\n",
       "      <td>0.793510</td>\n",
       "      <td>0.802289</td>\n",
       "      <td>0.794133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.416800</td>\n",
       "      <td>0.404027</td>\n",
       "      <td>0.812158</td>\n",
       "      <td>0.811516</td>\n",
       "      <td>0.814897</td>\n",
       "      <td>0.811545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.417700</td>\n",
       "      <td>0.381721</td>\n",
       "      <td>0.824316</td>\n",
       "      <td>0.824127</td>\n",
       "      <td>0.824887</td>\n",
       "      <td>0.824031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.391300</td>\n",
       "      <td>0.396565</td>\n",
       "      <td>0.829179</td>\n",
       "      <td>0.829057</td>\n",
       "      <td>0.831056</td>\n",
       "      <td>0.829656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.402500</td>\n",
       "      <td>0.358078</td>\n",
       "      <td>0.829179</td>\n",
       "      <td>0.829130</td>\n",
       "      <td>0.829193</td>\n",
       "      <td>0.829097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.386900</td>\n",
       "      <td>0.368451</td>\n",
       "      <td>0.832219</td>\n",
       "      <td>0.832115</td>\n",
       "      <td>0.832439</td>\n",
       "      <td>0.832036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.386400</td>\n",
       "      <td>0.346382</td>\n",
       "      <td>0.844377</td>\n",
       "      <td>0.844148</td>\n",
       "      <td>0.847724</td>\n",
       "      <td>0.845004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.332000</td>\n",
       "      <td>0.378373</td>\n",
       "      <td>0.819453</td>\n",
       "      <td>0.817772</td>\n",
       "      <td>0.834812</td>\n",
       "      <td>0.820813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.320900</td>\n",
       "      <td>0.404310</td>\n",
       "      <td>0.838906</td>\n",
       "      <td>0.838882</td>\n",
       "      <td>0.839609</td>\n",
       "      <td>0.839198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.293500</td>\n",
       "      <td>0.335259</td>\n",
       "      <td>0.849240</td>\n",
       "      <td>0.849179</td>\n",
       "      <td>0.849327</td>\n",
       "      <td>0.849122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.298600</td>\n",
       "      <td>0.368527</td>\n",
       "      <td>0.838906</td>\n",
       "      <td>0.838443</td>\n",
       "      <td>0.841335</td>\n",
       "      <td>0.838359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.305200</td>\n",
       "      <td>0.355183</td>\n",
       "      <td>0.846809</td>\n",
       "      <td>0.846788</td>\n",
       "      <td>0.847484</td>\n",
       "      <td>0.847094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.305900</td>\n",
       "      <td>0.437076</td>\n",
       "      <td>0.837082</td>\n",
       "      <td>0.834461</td>\n",
       "      <td>0.855869</td>\n",
       "      <td>0.835612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.294300</td>\n",
       "      <td>0.354196</td>\n",
       "      <td>0.846201</td>\n",
       "      <td>0.846142</td>\n",
       "      <td>0.846270</td>\n",
       "      <td>0.846090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.278200</td>\n",
       "      <td>0.350427</td>\n",
       "      <td>0.841337</td>\n",
       "      <td>0.841270</td>\n",
       "      <td>0.841434</td>\n",
       "      <td>0.841211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.306900</td>\n",
       "      <td>0.338040</td>\n",
       "      <td>0.845593</td>\n",
       "      <td>0.845592</td>\n",
       "      <td>0.845791</td>\n",
       "      <td>0.845754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.244400</td>\n",
       "      <td>0.398882</td>\n",
       "      <td>0.849848</td>\n",
       "      <td>0.849847</td>\n",
       "      <td>0.849920</td>\n",
       "      <td>0.849955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.215100</td>\n",
       "      <td>0.371027</td>\n",
       "      <td>0.852280</td>\n",
       "      <td>0.852243</td>\n",
       "      <td>0.852284</td>\n",
       "      <td>0.852217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.190500</td>\n",
       "      <td>0.445678</td>\n",
       "      <td>0.851672</td>\n",
       "      <td>0.851481</td>\n",
       "      <td>0.852521</td>\n",
       "      <td>0.851353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.222000</td>\n",
       "      <td>0.356815</td>\n",
       "      <td>0.853495</td>\n",
       "      <td>0.853495</td>\n",
       "      <td>0.853599</td>\n",
       "      <td>0.853619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.208600</td>\n",
       "      <td>0.376871</td>\n",
       "      <td>0.851672</td>\n",
       "      <td>0.851667</td>\n",
       "      <td>0.851685</td>\n",
       "      <td>0.851741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.181100</td>\n",
       "      <td>0.400782</td>\n",
       "      <td>0.849240</td>\n",
       "      <td>0.849240</td>\n",
       "      <td>0.849360</td>\n",
       "      <td>0.849371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.188600</td>\n",
       "      <td>0.391457</td>\n",
       "      <td>0.855927</td>\n",
       "      <td>0.855858</td>\n",
       "      <td>0.856071</td>\n",
       "      <td>0.855787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.168900</td>\n",
       "      <td>0.398449</td>\n",
       "      <td>0.851672</td>\n",
       "      <td>0.851637</td>\n",
       "      <td>0.851669</td>\n",
       "      <td>0.851617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.195700</td>\n",
       "      <td>0.402586</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.856965</td>\n",
       "      <td>0.857968</td>\n",
       "      <td>0.856832</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1389, training_loss=0.31007371783857296, metrics={'train_runtime': 1110.9229, 'train_samples_per_second': 39.98, 'train_steps_per_second': 1.25, 'total_flos': 1.16860775238144e+16, 'train_loss': 0.31007371783857296, 'epoch': 3.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
